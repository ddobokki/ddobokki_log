## 멘토링 
### [U] DL Basic 선택 과제 논문 리뷰
#### 1. ViT (Vision Transformer)  
  - NLP task의 standard. 기존의 transformer의 attention에 image fetch 개념으로 input data를 넣는다.  
  cnn + attention은 resnet보다 좋지 않았고, 그 이유는 combining 때문일거라고 논문에서 말함.  
  ViT model은 mid-size dataset 성능은 엄청 좋지는 않지만, larger dataset에서는 성능이 좋은데 논문에서는 inductive bias 때문이라고 말함. (보통 이미지 크기에 따라서 성능이 달라지지 않음)  
  참고 : inductive bias는 예측을 더 잘하기위한 부가적인 요소들. 기본 모델이 성능을 충분히 잘 낸다면 inductive bias가 모델 학습을 방해한다는 개념.  
  transformer 구조와 다른 점은 norm 위치를 바꾼것. (이게 성능 향상에 도움이 많이 되었다고 논문에 있음)   
  input : 이미지를 쪼개서 conv 넣은 후 flatten 후 하나의 단어처럼 ViT에 넣는다. 
  
#### 2. AAE (Adversarial Auto-Encoder)  
  - 확률모델. hidden code vector와 aggregated posterior를 매칭하는 모델  
  지금까지의 생성 모델은 MCMC 사용 (MCMC : 샘플링을 잘, 열심히 해서 원래의 모분포를 잘 묘사하는 방법)  
  현재는 VAE, GAN 같은 새로운 방법이 등장했는데, 학습하기가 어렵다는 단점이 있음.  
  VAE, GAN의 각각의 장점을 잘 살려보자는게 AEE의 핵심  
  
#### 3. MDN (Mixture Density Network)  
  - input : x / output : y  
  보통 데이터는 x는 여러개, y는 label 한개로 되어있음. 모델은 임의의 feature를 generalize 하는 방향으로 학습됨.   
  그런데 만약, x 하나에 y가 여러개라면? x와 y를 바꾸는 방법이 있음. input과 output을 바꿔서 데이터를 잘 표현할 수 있는 방법을 찾음.  
  y 분포에서 특정 x값을 찍고, x가 어떤 y 분포를 사용해서 표현할 수 있을까, 라는 고민을 하다가 여러개의 분포에서 값을 조금씩 가져온다면?   
  y만 있어도 x를 표현할 수 있지않을까,,, 라는 아이디어에서 시작된 논문.  
  input x를 넣고 neural network를 거치는 MDN을 사용했더니 결과가 잘 나온다.  
  (수식이 많은 논문...)
  
  
#### 질문
- 모델 개선방향 아이디어 얻는 방법? 
  1. 의문점을 해결하다가 새로운 아키텍처 발견
  2. 다른 task에서 잘 되는 방법을 적용하기
  
---
### 강의 & 과제 관련 질문
#### 과제를 다 수행하지 못했을 경우에 받는 패널티?  
  - 없다. 회사연계될 때도 활동내역이 넘어가지 않음.  
  그러니까 과제 제출보다는 지식을 얻는 것에 목표를 두자! (위로*123456789)
