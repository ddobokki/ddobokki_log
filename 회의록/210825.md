## 멘토링

- jupyter vs .py
  - jupyter
    - 1 gpu, multi-processing이 잘 안됨.(jupyter가 하나의 process여서 다른 process 불러야 하기 때문에)
    - gpu 해제하려면 종료 눌러야 해서 불편.
  - python script
    - 병렬화가 잘 됨
    - 분산 학습, 분산 처리가 쉽다.
    - gpu 자동 해제

### 고려해야 할 것

- wandb

  - 실험했던 기록 로깅 남겨줌. (팀 결성 후 적용 예정)
  - 비슷하게 neptune도 있다.

- Baseline code 짜기

  - 최소한의 수정으로 많은 실험할 수 있게

- 모델별로 최적화된 이미지 사이즈가 다르므로 실험 많이 해보기

- soft augmentation or hard augmentation

  - 성능은 hard가 더 좋긴 해서 메모리 공간이 되는 데까지 늘려라.
  - deterministic한 결과와 stochastic한 결과이기 때문에 섞을 수록 좋다.
  - 좌우반전은 무조건 좋다!
  - 얼굴이 반드시 포함된 random crop
  - 회전은 불확실
  - 상하반전 별로... 얼굴이 뒤집히면 당연히 안 좋다!

- validation

  - test, validation data set은 보통 원본 이미지. 얼굴 반드시 포함된 validation data set 보장할 수 있다면 넣어도 되긴 하지만 보통 원본 쓴다.
  - 보통 8:2. 7:3도 괜찮다.
  - `좋은 validation data set 만드는 것이 중요!!`

- 하나만 분류하는 모델을 18번 제출해보는 것도 ... 테스트 셋에서 어떤 라벨이 가장 많은지 체크하기 위해. training set의 분포를 test set 분포와 맞춰보려면 이 방법도 좋다! 하지만 private data set에서는 보장할 수 없음... 우선 해볼 수 있는 건 다 해보자.

- 실험 결과 공유해놓는 시트지 만들자. 여기에 실험을 토대로 insight 얻은 거를 써라 !!

- 역할 분배

- policy 정하기

- transform을 albumentation으로 바꿔라.

### 질문

- Q. 모델 각각 따로 training한 후 결과에서 합칠 때 좋을까? 아니면 한 번에 세 모델 training하는게 좋을까?
  - A. 컴퓨터 메모리에 따라 다르다! 하지만 대게 모델이 작으면 성능이 안좋기 때문에 trade-off가 있다. 실험을 다 해봐라!
- Q. 왜 augmentation 데이터가 validation data에 들어가면 안될까?
  -A. augmentation data는 가짜 data이기 때문에 가짜 data를 실험하게 되는 것이다. 내가 임의로 만든 데이터는 잘 맞출 가능성이 높다! 데이터가 엄청 크다면 넣어도 되긴 하는데... 흠

  augmentation 적은 데이터셋에 많이 하자!! 과적합을 방지하기 위해서는 필요. 우리가 찾고자 하는 global minima에 가까운 local minima 찾기 위해 augmentation은 데이터 분포를 골고루 펴주는 역할을 한다.
  한마디로 모델 관점에서는 모델을 위해 안정적으로 학습될 수 있게 해주는 역할.
  데이터 관점에서는 전체적인 데이터의 분포를 맞춰주는 역할.

  변형을 함으로 데이터의 오버피팅을 막아주고 데이터를 늘림으로 불균형을 맞춰 분포를 펴준다.

- Q. 인종이 다른 data set을 추가해도 영향이 없을까?
  -A. 얼굴을 이루고 있는 구조가 똑같으므로 잘 될 것이다! 테스트 해보고 싶으면 이 데이터를 validation에 넣어봐라.

### 추가 정보

- 최종 앙상블 방법 두 가지

  1. hard voting: 다수결.
  2. soft voting: 결과에 정확도를 곱함.

  - 성능이 낮은 경우도 넣어서 generalize되는 경우도 있다.

- augmentation 위치 중요하다. 만약 이 코드 이후부터는 이미지가 변할 일이 없다면 resizing 밑에 augmentation 코드를 넣어라! 일 두 번하게 되니까.

- ViT 계열 보통 transfer 잘 안되긴 하는데 데이터가 많다면 시도해볼 수 있다.

- Grad-CAM 같은 거로 마스크 잘 쓰고 있는지 판단해볼 수도...

- pre-trained model 쓰려면 얼굴 관련 task했던 weight 가져오면 좋을 듯.

- age issue에서 anomaly detection도 하나의 방법. 60세 이상은 비정상. 미만은 정상.

- training accuracy가 빠르게 증가하면 validation accuracy에서도 빠르게 되어야 트레이닝이 잘 되는 것. 둘 다 최대한 데이터 많이 늘리길!!

- epoch도 많이 돌려 놓길! loss값 보고 학습 결과 저장해라. 2-3 epoch마다 validation 돌리고 checkpoint 저장!

## 저녁 meeting

- [스프레드 시트](https://docs.google.com/spreadsheets/d/1NlhhOr35Yv5Z9rir1ta-zivudJg3wpjQPY8KQ2EkWps/edit#gid=0)
- [구글 독스](https://docs.google.com/document/d/1bDi4Yy34RhbA_fvE1L6jC37erN9nys1FFq3TXnpM_ws/edit)
